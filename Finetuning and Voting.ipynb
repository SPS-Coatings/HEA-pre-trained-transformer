{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4a553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tensorboard  (optional)\n",
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir runs/train\n",
    "%tensorboard --logdir ./logs\n",
    "#%tensorboard --logdir {logs_base_dir}  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04055ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################ Fine Tune ##################################\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import TrainerCallback, IntervalStrategy\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "import os\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import re\n",
    "\n",
    "\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([f\"{element}{fraction}\" for element, fraction in sorted_matches])\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Final_Formated_and_cleaned_file_No_Features.csv')\n",
    "\n",
    "# Tokenize and normalize\n",
    "data['tokenized_elements'] = data['composition'].apply(custom_tokenize)\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_phase'] = label_encoder.fit_transform(data['Phase'])\n",
    "\n",
    "feature_columns = [col for col in data.columns if col not in ['composition', 'Phase', 'tokenized_elements', 'encoded_phase']]\n",
    "for feature in feature_columns:\n",
    "    scaler = StandardScaler()\n",
    "    data[f'normalized_{feature}'] = scaler.fit_transform(data[[feature]])\n",
    "\n",
    "data['combined_features'] = data['tokenized_elements'] + ' ' + data[[f'normalized_{feature}' for feature in feature_columns]].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Split data\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./results/pretrained_BERT_1M')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./results/pretrained_BERT_1M', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "train_encodings = tokenizer(data_train['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(data_test['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, data_train['encoded_phase'].values)\n",
    "test_dataset = CustomDataset(test_encodings, data_test['encoded_phase'].values)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "# Initialize custom optimizer with weight decay for specific layers\n",
    "decay_layers = [\"10\", \"11\", \"12\"]\n",
    "decay_param_names = [n for n, p in model.named_parameters() if any(f\".{layer}.\" in n for layer in decay_layers)]\n",
    "no_decay_param_names = [n for n, p in model.named_parameters() if n not in decay_param_names]\n",
    "decay_params = [p for n, p in model.named_parameters() if n in decay_param_names]\n",
    "no_decay_params = [p for n, p in model.named_parameters() if n in no_decay_param_names]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": decay_params, \"weight_decay\": 0.01},\n",
    "    {\"params\": no_decay_params, \"weight_decay\": 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = len(train_dataset) * 30\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = './logs/' + current_time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=12,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=1,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "class_report_str = classification_report(predictions.label_ids, pred_labels, target_names=label_encoder.classes_)\n",
    "print(\"Classification Report:\\n\", class_report_str)\n",
    "\n",
    "with open('./results/classification_report.txt', 'w') as f:\n",
    "    f.write(class_report_str)\n",
    "\n",
    "conf_mat = confusion_matrix(predictions.label_ids, pred_labels)\n",
    "print(\"Confusion Matrix:\", conf_mat)\n",
    "\n",
    "with open('./results/confusion_matrix.txt', 'w') as f:\n",
    "    np.savetxt(f, conf_mat, fmt='%d')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('./results/confusion_matrix.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "tokenizer.save_pretrained('./results')\n",
    "model.save_pretrained('./results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a7545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
