{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### Dataset Analysis ############################\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Final_Formated_and_cleaned_file_With_Features.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to get element count in a composition\n",
    "def count_elements(composition):\n",
    "    return len(re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition))\n",
    "\n",
    "# Get unique phases and their counts\n",
    "phase_counts = df['Phase'].value_counts()\n",
    "\n",
    "# Get the percentage of each phase\n",
    "total_instances = len(df)\n",
    "phase_percentage = (phase_counts / total_instances) * 100\n",
    "\n",
    "# Get the phase with the maximum instances\n",
    "max_phase = phase_counts.idxmax()\n",
    "max_phase_count = phase_counts.max()\n",
    "\n",
    "# Get the total number of features (excluding 'composition' and 'Phase' columns)\n",
    "total_features = df.shape[1] - 2\n",
    "\n",
    "# Get the number of compositions containing N elements\n",
    "composition_counts = df['composition'].apply(count_elements).value_counts().sort_index()\n",
    "\n",
    "phase_counts, phase_percentage, total_instances, max_phase, max_phase_count, total_features, composition_counts\n",
    "\n",
    "\n",
    "# Write the statistics to a text file\n",
    "statistics_text = \"\"\"\n",
    "### Dataset Statistics\n",
    "\n",
    "#### General Information\n",
    "- **Total Instances**: 1184\n",
    "- **Total Features**: 14\n",
    "\n",
    "#### Phase Statistics\n",
    "- **Unique Phases**: 8\n",
    "  - **BCC**: 386 instances (32.60%)\n",
    "  - **FCC**: 352 instances (29.73%)\n",
    "  - **BCC+Sec**: 123 instances (10.39%)\n",
    "  - **FCC+Sec**: 99 instances (8.36%)\n",
    "  - **FCC+BCC**: 71 instances (6.00%)\n",
    "  - **FCC+BCC+Sec**: 58 instances (4.90%)\n",
    "  - **HCP**: 54 instances (4.56%)\n",
    "  - **Sec**: 41 instances (3.46%)\n",
    "\n",
    "- **Phase with Maximum Instances**: BCC (386 instances)\n",
    "\n",
    "#### Composition Statistics\n",
    "- **Compositions Containing N Elements**:\n",
    "  - **2 Elements**: 425 compositions\n",
    "  - **3 Elements**: 70 compositions\n",
    "  - **4 Elements**: 86 compositions\n",
    "  - **5 Elements**: 315 compositions\n",
    "  - **6 Elements**: 221 compositions\n",
    "  - **7 Elements**: 59 compositions\n",
    "  - **8 Elements**: 4 compositions\n",
    "  - **9 Elements**: 4 compositions\n",
    "\"\"\"\n",
    "\n",
    "# Save to a text file\n",
    "statistics_file_path = 'dataset_statistics.txt'\n",
    "with open(statistics_file_path, 'w') as f:\n",
    "    f.write(statistics_text)\n",
    "\n",
    "statistics_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded321a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard  (optional)\n",
    "#sto comand prompt\n",
    "#taskkill /im tensorboard.exe /f\n",
    "#del /q %TMP%\\.tensorboard-info\\*\n",
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir runs/train\n",
    "%tensorboard --logdir ./logs\n",
    "#%tensorboard --logdir {logs_base_dir}  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf35a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################  Esembled Model All Layers Fine Tune  #############################\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import TrainerCallback, IntervalStrategy\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "import os\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import re\n",
    "\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([f\"{element}{fraction}\" for element, fraction in sorted_matches])\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Final_Formated_and_cleaned_file_No_Features.csv')\n",
    "\n",
    "# Tokenize and normalize\n",
    "data['tokenized_elements'] = data['composition'].apply(custom_tokenize)\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_phase'] = label_encoder.fit_transform(data['Phase'])\n",
    "\n",
    "feature_columns = [col for col in data.columns if col not in ['composition', 'Phase', 'tokenized_elements', 'encoded_phase']]\n",
    "for feature in feature_columns:\n",
    "    scaler = StandardScaler()\n",
    "    data[f'normalized_{feature}'] = scaler.fit_transform(data[[feature]])\n",
    "\n",
    "data['combined_features'] = data['tokenized_elements'] + ' ' + data[[f'normalized_{feature}' for feature in feature_columns]].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Split data\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./results/pretrained_BERT_200k')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./results/pretrained_BERT_200k', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Tokenize\n",
    "train_encodings = tokenizer(data_train['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(data_test['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, data_train['encoded_phase'].values)\n",
    "test_dataset = CustomDataset(test_encodings, data_test['encoded_phase'].values)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "# optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_dataset) * 30\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = './logs/' + current_time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='d:',\n",
    "    num_train_epochs=9,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=1,\n",
    "    save_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "class_report_str = classification_report(predictions.label_ids, pred_labels, target_names=label_encoder.classes_)\n",
    "print(\"Classification Report:\\n\", class_report_str)\n",
    "\n",
    "with open('./results/classification_report.txt', 'w') as f:\n",
    "    f.write(class_report_str)\n",
    "\n",
    "conf_mat = confusion_matrix(predictions.label_ids, pred_labels)\n",
    "print(\"Confusion Matrix:\", conf_mat)\n",
    "\n",
    "with open('./results/confusion_matrix.txt', 'w') as f:\n",
    "    np.savetxt(f, conf_mat, fmt='%d')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('./results/confusion_matrix.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "tokenizer.save_pretrained('./results')\n",
    "model.save_pretrained('./results')\n",
    "\n",
    "\n",
    "#################################### Random Forest #######################################\n",
    "# Load the data\n",
    "df = pd.read_csv('Final_Formated_and_cleaned_file_With_Features.csv')\n",
    "# df = pd.read_csv('Fine Tuning Medium and HEA_Rounded_Cleaned_Acoustics.csv')\n",
    "\n",
    "def get_element_fraction(composition, element):\n",
    "    if element in composition:\n",
    "        # Extract the portion of the string after the element's name\n",
    "        remainder = composition[composition.index(element) + len(element):]\n",
    "        \n",
    "        # Extract the coefficient using regex\n",
    "        import re\n",
    "        match = re.search(r\"(\\d+(\\.\\d+)?)\", remainder)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return 0.0\n",
    "\n",
    "# Create columns for each metallic element and fill with its coefficient\n",
    "METALLIC_ELEMENTS = [\"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\",\n",
    "    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"Se\",\n",
    "    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Te\",\n",
    "    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\",\n",
    "    \"Yb\", \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Tl\", \"Pb\", \"Po\", \"Th\", \"Pa\", \"U\"]\n",
    "\n",
    "for element in METALLIC_ELEMENTS:\n",
    "    df[element] = df['composition'].apply(lambda x: get_element_fraction(x, element))\n",
    "\n",
    "\n",
    "# Drop the original 'composition' column and 'hardness' column\n",
    "X = df.drop(columns=['composition', 'Phase'])\n",
    "y = df['Phase']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=60, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save the Random Forest model\n",
    "joblib.dump(clf, './results/random_forest_model.pkl')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# BERT Predictions\n",
    "bert_predictions = trainer.predict(test_dataset)\n",
    "bert_pred_labels = np.argmax(bert_predictions.predictions, axis=1)\n",
    "bert_pred_probs = scipy.special.softmax(bert_predictions.predictions, axis=1)  # Converting logits to probabilities\n",
    "\n",
    "# Transform BERT labels back to original (string) labels\n",
    "bert_pred_labels_str = label_encoder.inverse_transform(bert_pred_labels)\n",
    "\n",
    "# Random Forest Predictions\n",
    "rf_pred_labels = clf.predict(X_test)\n",
    "rf_pred_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# Make sure that both y_test and final_predictions are of the same type (either both numbers or both strings)\n",
    "# Here, converting everything to string type\n",
    "y_test_str = y_test.astype(str)\n",
    "rf_pred_labels_str = rf_pred_labels.astype(str)\n",
    "\n",
    "# Weighted Voting\n",
    "bert_weight = 0.3\n",
    "rf_weight = 0.7\n",
    "final_predictions_weighted = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    avg_prob = b_prob * bert_weight + r_prob * rf_weight\n",
    "    final_prediction = np.argmax(avg_prob)\n",
    "    final_predictions_weighted.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Soft Voting with Probabilities\n",
    "final_predictions_soft = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    avg_prob = (b_prob + r_prob) / 2\n",
    "    final_prediction = np.argmax(avg_prob)\n",
    "    final_predictions_soft.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Confidence-based Voting\n",
    "final_predictions_confidence = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    b_confidence = np.max(b_prob)\n",
    "    r_confidence = np.max(r_prob)\n",
    "    final_prediction = np.argmax(b_prob) if b_confidence > r_confidence else np.argmax(r_prob)\n",
    "    final_predictions_confidence.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Assume final_predictions holds the ensemble predictions you want to evaluate\n",
    "final_predictions = final_predictions_confidence  # or final_predictions_soft or final_predictions_confidence\n",
    "\n",
    "# Generate the confusion matrix\n",
    "final_cm = confusion_matrix(y_test_str, final_predictions)\n",
    "\n",
    "# Calculate accuracy from the confusion matrix\n",
    "final_accuracy = np.trace(final_cm) / np.sum(final_cm)\n",
    "\n",
    "# Print the calculated accuracy\n",
    "print(\"Final Model Accuracy:\", final_accuracy)\n",
    "\n",
    "# Debug Step 4: Check Confusion Matrix Labels\n",
    "sorted_labels = sorted(y_test.unique())\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(final_cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=sorted_labels, yticklabels=sorted_labels)\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Ensemble Model Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the different ensemble methods\n",
    "print(\"Weighted Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_weighted))\n",
    "print(\"Soft Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_soft))\n",
    "print(\"Confidence-based Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_confidence))\n",
    "\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nEnsemble Model Classification Report:\\n\", classification_report(y_test, final_predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd63259",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Frozen Layers Training ############################\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import TrainerCallback, IntervalStrategy\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "import os\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import re\n",
    "\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([f\"{element}{fraction}\" for element, fraction in sorted_matches])\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Final_Formated_and_cleaned_file_With_Features_Rounded_Cleaned_Acoustics.csv')\n",
    "\n",
    "# Tokenize and normalize\n",
    "data['tokenized_elements'] = data['composition'].apply(custom_tokenize)\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_phase'] = label_encoder.fit_transform(data['Phase'])\n",
    "\n",
    "feature_columns = [col for col in data.columns if col not in ['composition', 'Phase', 'tokenized_elements', 'encoded_phase']]\n",
    "for feature in feature_columns:\n",
    "    scaler = StandardScaler()\n",
    "    data[f'normalized_{feature}'] = scaler.fit_transform(data[[feature]])\n",
    "\n",
    "data['combined_features'] = data['tokenized_elements'] + ' ' + data[[f'normalized_{feature}' for feature in feature_columns]].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Split data\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./results/pretrained_BERT_200K')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./results/pretrained_BERT_200K', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "train_encodings = tokenizer(data_train['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(data_test['combined_features'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, data_train['encoded_phase'].values)\n",
    "test_dataset = CustomDataset(test_encodings, data_test['encoded_phase'].values)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "# Initialize custom optimizer with weight decay for specific layers\n",
    "decay_layers = [\"1\", \"5\", \"9\", \"12\"]\n",
    "decay_param_names = [n for n, p in model.named_parameters() if any(f\".{layer}.\" in n for layer in decay_layers)]\n",
    "no_decay_param_names = [n for n, p in model.named_parameters() if n not in decay_param_names]\n",
    "decay_params = [p for n, p in model.named_parameters() if n in decay_param_names]\n",
    "no_decay_params = [p for n, p in model.named_parameters() if n in no_decay_param_names]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": decay_params, \"weight_decay\": 0.04},\n",
    "    {\"params\": no_decay_params, \"weight_decay\": 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = len(train_dataset) * 30\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = './logs/' + current_time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='d:',\n",
    "    num_train_epochs=13,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=1,\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "class_report_str = classification_report(predictions.label_ids, pred_labels, target_names=label_encoder.classes_)\n",
    "print(\"Classification Report:\\n\", class_report_str)\n",
    "\n",
    "with open('./results/classification_report.txt', 'w') as f:\n",
    "    f.write(class_report_str)\n",
    "\n",
    "conf_mat = confusion_matrix(predictions.label_ids, pred_labels)\n",
    "print(\"Confusion Matrix:\", conf_mat)\n",
    "\n",
    "with open('./results/confusion_matrix.txt', 'w') as f:\n",
    "    np.savetxt(f, conf_mat, fmt='%d')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('./results/confusion_matrix.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "tokenizer.save_pretrained('d:')\n",
    "model.save_pretrained('d:')\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Final_Formated_and_cleaned_file_With_Features_Rounded_Cleaned_Acoustics.csv')\n",
    "# df = pd.read_csv('Fine Tuning Medium and HEA_Rounded_Cleaned.csv')\n",
    "\n",
    "def get_element_fraction(composition, element):\n",
    "    if element in composition:\n",
    "        # Extract the portion of the string after the element's name\n",
    "        remainder = composition[composition.index(element) + len(element):]\n",
    "        \n",
    "        # Extract the coefficient using regex\n",
    "        import re\n",
    "        match = re.search(r\"(\\d+(\\.\\d+)?)\", remainder)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return 0.0\n",
    "\n",
    "# Create columns for each metallic element and fill with its coefficient\n",
    "METALLIC_ELEMENTS = [\"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\",\n",
    "    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"Se\",\n",
    "    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Te\",\n",
    "    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\",\n",
    "    \"Yb\", \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Tl\", \"Pb\", \"Po\", \"Th\", \"Pa\", \"U\"]\n",
    "\n",
    "for element in METALLIC_ELEMENTS:\n",
    "    df[element] = df['composition'].apply(lambda x: get_element_fraction(x, element))\n",
    "\n",
    "\n",
    "# Drop the original 'composition' column and 'hardness' column\n",
    "X = df.drop(columns=['composition', 'Phase'])\n",
    "y = df['Phase']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save the Random Forest model\n",
    "joblib.dump(clf, './results/random_forest_model.pkl')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# BERT Predictions\n",
    "bert_predictions = trainer.predict(test_dataset)\n",
    "bert_pred_labels = np.argmax(bert_predictions.predictions, axis=1)\n",
    "bert_pred_probs = scipy.special.softmax(bert_predictions.predictions, axis=1)  # Converting logits to probabilities\n",
    "\n",
    "# Transform BERT labels back to original (string) labels\n",
    "bert_pred_labels_str = label_encoder.inverse_transform(bert_pred_labels)\n",
    "\n",
    "# Random Forest Predictions\n",
    "rf_pred_labels = clf.predict(X_test)\n",
    "rf_pred_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# Make sure that both y_test and final_predictions are of the same type (either both numbers or both strings)\n",
    "# Here, converting everything to string type\n",
    "y_test_str = y_test.astype(str)\n",
    "rf_pred_labels_str = rf_pred_labels.astype(str)\n",
    "\n",
    "# Weighted Voting\n",
    "bert_weight = 0.3\n",
    "rf_weight = 0.7\n",
    "final_predictions_weighted = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    avg_prob = b_prob * bert_weight + r_prob * rf_weight\n",
    "    final_prediction = np.argmax(avg_prob)\n",
    "    final_predictions_weighted.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Soft Voting with Probabilities\n",
    "final_predictions_soft = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    avg_prob = (b_prob + r_prob) / 2\n",
    "    final_prediction = np.argmax(avg_prob)\n",
    "    final_predictions_soft.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Confidence-based Voting\n",
    "final_predictions_confidence = []\n",
    "\n",
    "for b_prob, r_prob in zip(bert_pred_probs, rf_pred_probs):\n",
    "    b_confidence = np.max(b_prob)\n",
    "    r_confidence = np.max(r_prob)\n",
    "    final_prediction = np.argmax(b_prob) if b_confidence > r_confidence else np.argmax(r_prob)\n",
    "    final_predictions_confidence.append(label_encoder.classes_[final_prediction])\n",
    "\n",
    "# Assume final_predictions holds the ensemble predictions you want to evaluate\n",
    "final_predictions = final_predictions_weighted  # or final_predictions_soft or final_predictions_confidence\n",
    "\n",
    "# Generate the confusion matrix\n",
    "final_cm = confusion_matrix(y_test_str, final_predictions)\n",
    "\n",
    "# Calculate accuracy from the confusion matrix\n",
    "final_accuracy = np.trace(final_cm) / np.sum(final_cm)\n",
    "\n",
    "# Print the calculated accuracy\n",
    "print(\"Final Model Accuracy:\", final_accuracy)\n",
    "\n",
    "\n",
    "# Debug Step 4: Check Confusion Matrix Labels\n",
    "sorted_labels = sorted(y_test.unique())\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(final_cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=sorted_labels, yticklabels=sorted_labels)\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Ensemble Model Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the different ensemble methods\n",
    "print(\"Weighted Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_weighted))\n",
    "print(\"Soft Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_soft))\n",
    "print(\"Confidence-based Voting Accuracy:\", accuracy_score(y_test_str, final_predictions_confidence))\n",
    "\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nEnsemble Model Classification Report:\\n\", classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cdfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################  Heat Maps ############################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "\n",
    "# Function to tokenize and sort elements\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([''.join(pair) for pair in sorted_matches])\n",
    "\n",
    "# Load your trained model and tokenizer\n",
    "model_path = './results'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, output_attentions=True)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Original composition\n",
    "composition = \"Co1 Cr1 Fe1 Mn1 Ni1 V1\"\n",
    "tokenized_composition = custom_tokenize(composition)\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(tokenized_composition, return_tensors=\"pt\", add_special_tokens=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Get Attention Weights\n",
    "outputs = model(**inputs)\n",
    "attentions = outputs.attentions  # List of attention weights for each layer\n",
    "\n",
    "# Aggregate across all heads for simplicity (could be refined)\n",
    "avg_attention = attentions[-1].squeeze(0).mean(0).detach().numpy()\n",
    "\n",
    "# Exclude the [CLS] and [SEP] tokens\n",
    "avg_attention = avg_attention[1:-1, 1:-1]\n",
    "\n",
    "# Average the attention values for element pairs\n",
    "element_count = len(re.findall(r'([A-Z][a-z]*[0-9.]+)', tokenized_composition))\n",
    "avg_attention_relevant = np.zeros((element_count, element_count))\n",
    "\n",
    "for i in range(0, avg_attention.shape[0], 2):\n",
    "    for j in range(0, avg_attention.shape[1], 2):\n",
    "        avg_value = np.mean(avg_attention[i:i+2, j:j+2])\n",
    "        avg_attention_relevant[i//2, j//2] = avg_value\n",
    "\n",
    "# Symmetrize the matrix\n",
    "avg_attention_symmetric = (avg_attention_relevant + avg_attention_relevant.T) / 2\n",
    "\n",
    "# Visualize\n",
    "labels = custom_tokenize(composition).split()\n",
    "sns.heatmap(avg_attention_symmetric, annot=True, xticklabels=labels, yticklabels=labels)\n",
    "plt.show()\n",
    "\n",
    "# Visualize\n",
    "labels = custom_tokenize(composition).split()\n",
    "sns.heatmap(avg_attention_symmetric, annot=True, xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"heatmap_300dpi.png\", dpi=300)  # Saving with 300dpi resolution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016abb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################### LIME   ###########################\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Final_Formated_and_cleaned_file_No_Features.csv')\n",
    "\n",
    "# Function to tokenize and sort elements\n",
    "def custom_tokenize(composition):\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    return ' '.join([''.join(pair) for pair in sorted_matches])\n",
    "\n",
    "# Apply the function to the data\n",
    "data['tokenized_elements'] = data['composition'].apply(custom_tokenize)\n",
    "\n",
    "# Label encode the 'Phase' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_phase'] = label_encoder.fit_transform(data['Phase'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "model_path = './results'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load trained Random Forest model\n",
    "clf = joblib.load('./results/random_forest_model.pkl')\n",
    "\n",
    "# Function to predict probabilities using the ensemble model\n",
    "def predictor(texts):\n",
    "    # Prepare data for BERT\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    bert_probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "    # Prepare data for Random Forest\n",
    "    METALLIC_ELEMENTS = [\"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\",\n",
    "    \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"Se\",\n",
    "    \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Te\",\n",
    "    \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\",\n",
    "    \"Yb\", \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Tl\", \"Pb\", \"Po\", \"Th\", \"Pa\", \"U\"]\n",
    "    \n",
    "    def get_element_fraction(composition, element):\n",
    "        if element in composition:\n",
    "            remainder = composition[composition.index(element):]\n",
    "            match = re.search(r\"(\\d+(\\.\\d+)?)\", remainder)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        return 0.0\n",
    "\n",
    "    X_rf = np.array([[get_element_fraction(text, el) for el in METALLIC_ELEMENTS] for text in texts])\n",
    "    \n",
    "    # Get Random Forest probabilities\n",
    "    rf_probs = clf.predict_proba(X_rf)\n",
    "\n",
    "    # Ensemble probabilities\n",
    "    bert_weight = 0.3\n",
    "    rf_weight = 0.7\n",
    "    ensemble_probs = bert_probs * bert_weight + rf_probs * rf_weight\n",
    "\n",
    "    return ensemble_probs\n",
    "\n",
    "# Function to split text for LIME\n",
    "def custom_split(text):\n",
    "    return re.findall(r'([A-Z][a-z]*[0-9.]+)', text)\n",
    "\n",
    "# Example to be explained\n",
    "your_composition = \"Co1 Cr1 Fe1 Mn1 Ni0.8 V1\"\n",
    "sorted_composition = custom_tokenize(your_composition)\n",
    "sample_text = sorted_composition\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = LimeTextExplainer(class_names=label_encoder.classes_, split_expression=custom_split)\n",
    "\n",
    "# Explain the instance\n",
    "explanation = explainer.explain_instance(sample_text, predictor, num_features=len(label_encoder.classes_), top_labels=3)\n",
    "\n",
    "# Show and save the explanation\n",
    "explanation.show_in_notebook(text=sample_text)\n",
    "explanation.save_to_file('A_explanation_output.html')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract LIME figures for each class label of interest\n",
    "for label in explanation.top_labels:\n",
    "    fig = explanation.as_pyplot_figure(label=label)\n",
    "    plt.savefig(f\"lime_explanation_{label_encoder.classes_[label]}.png\", dpi=300)\n",
    "    plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
